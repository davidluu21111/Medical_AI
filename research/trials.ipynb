{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract information from a pdf \n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaded = load_pdf(data=\"DATA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into chunks \n",
    "\n",
    "def text_split(data_loaded):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    text_chunks = text_splitter.split_documents(data_loaded)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks= text_split(data_loaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the embeddings from hugging face\n",
    "def download_hugging_face_embed():\n",
    "    embeddings= HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khoil\\AppData\\Local\\Temp\\ipykernel_37440\\3765694230.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings= HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
      "c:\\Users\\khoil\\anaconda3\\envs\\medibot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\khoil\\anaconda3\\envs\\medibot\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\khoil\\anaconda3\\envs\\medibot\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result =  embeddings.embed_query(\"Hello World\")\n",
    "print(\"Length\", len (query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "GROQ_API_KEY = os.environ.get('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"medicalbot\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"medicalbot-gw3p5a1.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY)\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed chunk and upsert embedding into Pinecone index\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents (\n",
    "    documents = text_chunks,\n",
    "    index_name = index_name,\n",
    "    embedding= embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load index\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= docsearch.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke (\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c59cd73e-41cc-41aa-92e8-4c582b7fb817', metadata={'creationdate': '2006-10-16T20:19:33+02:00', 'creator': 'Adobe Acrobat 6.0', 'moddate': '2006-10-16T22:03:45+02:00', 'page': 54.0, 'page_label': '25', 'producer': 'PDFlib+PDI 6.0.3 (SunOS)', 'source': 'DATA\\\\medical_book.pdf', 'total_pages': 4505.0}, page_content='Acne vulgaris, the medical term for common acne,\\nis the most common skin disease. It affects nearly 17\\nmillion people in the United States. While acne can\\narise at any age, it usually begins atpuberty and wor-\\nsens during adolescence. Nearly 85% of people\\ndevelop acne at some time between the ages of 12-25\\nyears. Up to 20% of women develop mild acne. It is\\nalso found in some newborns.\\nThe sebaceous glands lie just beneath the skin’s\\nsurface. They produce an oil called sebum, the skin’s'),\n",
       " Document(id='5766d21f-b1f0-4ea9-8562-11d6972d0ddd', metadata={'creationdate': '2006-10-16T20:19:33+02:00', 'creator': 'Adobe Acrobat 6.0', 'moddate': '2006-10-16T22:03:45+02:00', 'page': 54.0, 'page_label': '25', 'producer': 'PDFlib+PDI 6.0.3 (SunOS)', 'source': 'DATA\\\\medical_book.pdf', 'total_pages': 4505.0}, page_content='Pathological Stage and Recurrence in Radical\\nProstatectomy Cases.’’Journal of Urology (March\\n1998): 935-940.\\nNancy J. Nordenson\\nAcid reflux see Heartburn\\nAcidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when\\nthe pores of the skin become clogged with oil, dead\\nskin cells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne,'),\n",
       " Document(id='5a25b0bb-49ae-4058-9a61-85577177edb4', metadata={'creationdate': '2006-10-16T20:19:33+02:00', 'creator': 'Adobe Acrobat 6.0', 'moddate': '2006-10-16T22:03:45+02:00', 'page': 54.0, 'page_label': '25', 'producer': 'PDFlib+PDI 6.0.3 (SunOS)', 'source': 'DATA\\\\medical_book.pdf', 'total_pages': 4505.0}, page_content='blackheads.\\nModerate and severe inflammatory types of acne\\nresult after the plugged follicle is invaded by\\nPropionibacterium acnes, a bacteria that normally\\nlives on the skin. A pimple forms when the damaged\\nfollicle weakens and bursts open, releasing sebum,\\nbacteria, and skin and white blood cells into the sur-\\nrounding tissues. Inflamed pimples near the skin’s sur-\\nface are called papules; when deeper, they are called\\npustules. The most severe type of acne consists of cysts')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "# 1) Install needed libraries:\n",
    "#    pip install groq requests langchain pydantic\n",
    "\n",
    "from groq import Groq\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class OfficialGroqLLM(LLM, BaseModel):\n",
    "    \"\"\"\n",
    "    A LangChain-compatible LLM wrapper that calls Groq's official Python client.\n",
    "    \"\"\"\n",
    "\n",
    "    api_key: str = Field(..., description=\"Your Groq API key\")\n",
    "    model: str = Field(\"llama-3.3-70b-versatile\", description=\"Groq model name\")\n",
    "    temperature: float = Field(0.4, description=\"Sampling temperature\")\n",
    "    max_tokens: int = Field(500, description=\"Maximum tokens to generate\")\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"groq_official\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Sends a single 'user' message to Groq. If you need a 'system' message,\n",
    "        incorporate it into the prompt or customize messages below.\n",
    "        \"\"\"\n",
    "        # Initialize the Groq client\n",
    "        client = Groq(api_key=self.api_key)\n",
    "\n",
    "        # Convert the LangChain prompt into Groq's chat format\n",
    "        # You can add a system message if desired:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        # Call the official Groq API\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens\n",
    "            # Groq might support additional parameters if needed\n",
    "        )\n",
    "\n",
    "        # Return the text from the first choice\n",
    "        return chat_completion.choices[0].message.content\n",
    "\n",
    "    async def _acall(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        raise NotImplementedError(\"Async method not implemented for OfficialGroqLLM.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "groq_llm = OfficialGroqLLM(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\", \"YOUR_GROQ_KEY_HERE\"),\n",
    "    model=\"llama-3.3-70b-versatile\", \n",
    "    temperature=0.4,\n",
    "    max_tokens=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use five sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(groq_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what \"stats\" refers to in this context. The provided information discusses psychological testing and terms related to it, but it does not define \"stats.\" If you meant to ask about statistics, I can tell you that it involves the study of data collection and analysis, but this is not explicitly mentioned in the given context. Without more information, I cannot provide a specific answer.\n"
     ]
    }
   ],
   "source": [
    "respone = rag_chain.invoke ({\"input\": \"What is stats?\"})\n",
    "print (respone[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
